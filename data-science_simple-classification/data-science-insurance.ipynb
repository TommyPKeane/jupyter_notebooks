{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Data Science (Insurance)\" Simple Challenge\n",
    "\n",
    "We found a company's publicly-hosted GitHub Account with simple pre-interview challenges for several Data-Science-related roles. We removed the company information and created this notebook to practice the challenges.\n",
    "\n",
    "The data is theirs, but all code is our creation. The repository was provided under the MIT License, so while technically we've broken the license by not including it, we'd rather keep the anonymity and disassociation since the MIT License is free and permissive.\n",
    "\n",
    "All dependencies (`import` calls) for this notebook are in the first code-block.\n",
    "\n",
    "Alongside this notebook is a lock-file generated using the `poetry` package, which has all the exact dependencies used to generate and run this Notebook. The `pyproject.toml` file is the project configuration file generated by `poetry` -- which is installable with `pip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Challenge\n",
    "\n",
    "<style type=\"text/css\">\n",
    "    ol ol { list-style-type: lower-alpha; }\n",
    "</style>\n",
    "\n",
    "_(The following has been extracted from a PDF at the originating company's GitHub repository of challenges. The associated data with the challenge are in the `data-science-marketing_data` directory.)_\n",
    "\n",
    "Success in the simple challenge leads to the final two steps of the interview process:\n",
    "\n",
    "1. Informal chat with [Company] founders\n",
    "1. Full technical challenge\n",
    "\n",
    "For the simple challenge, use \"train.csv\" to predict the `outcome` variable using a __Generalised Linear Model__ (GLM) with a _log_ Link Function and Poisson Distribution. The category named `categorical` is a categorical column and the column named `numeric` is a numeric column. Both should be used as independent variables.\n",
    "\n",
    "__Requirements:__\n",
    "\n",
    "1. All code must be written in Python and must be in a Jupyter notebook.\n",
    "1. The first cell in the notebook must include:\n",
    "  1. Your last name (please don’t include any other identifying information)\n",
    "  1. The date\n",
    "1. You must output the GLM's parameter estimates.\n",
    "1. Your code must be able to predict all five observations in the \"test.csv\" dataset. The last cell in the notebook must output the five predicted values of the `outcome` variable for \"test.csv\".\n",
    "1. A key point of evaluation is how well written the code is. Please write the code as if you are writing it for a production setting. No need to wrap the code in a service or write Dockerfiles. Just ensure the code you write is not hacked together.\n",
    "\n",
    "If you are spending more than an hour on this simple challenge because there are so many things you want to demonstrate, you are spending too much time on it. If you are spending more than an hour on it because you don’t know where to start, please be warned that the full technical challenge will be considerably more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Python v3.8.2 (default, Apr 13 2020, 19:02:26) \n",
      "[Clang 11.0.3 (clang-1103.0.32.29)]\n",
      "\n",
      ">> Loading: pandas v1.1.0\n",
      ">> Loading: plotly v4.9.0\n",
      ">> Loading: sklearn v0.23.2\n",
      "\n",
      ">> Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Python Standard Library\n",
    "import pathlib;\n",
    "import sys;\n",
    "\n",
    "# Third-Party Packages\n",
    "import pandas;\n",
    "\n",
    "import plotly;\n",
    "import plotly.express as plotly_express;\n",
    "\n",
    "import sklearn;\n",
    "\n",
    "print(\">> Python v{0:s}\".format(sys.version));\n",
    "print(\"\");\n",
    "print(\">> Loading: pandas v{0:s}\".format(pandas.__version__));\n",
    "print(\">> Loading: plotly v{0:s}\".format(plotly.__version__));\n",
    "print(\">> Loading: sklearn v{0:s}\".format(sklearn.__version__));\n",
    "print(\"\");\n",
    "print(\">> Dependencies Loaded.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      ">  Size: (195, 3)\n",
      "    outcome  categorical  numeric\n",
      "0         0          3.0     41.0\n",
      "1         0          1.0     41.0\n",
      "2         0          3.0     44.0\n",
      "3         0          3.0      NaN\n",
      "4         0          NaN     40.0\n",
      "5         0          1.0     42.0\n",
      "6         0          3.0     46.0\n",
      "7         0          NaN     40.0\n",
      "8         0          3.0     33.0\n",
      "9         0          3.0     46.0\n",
      "10        0          3.0     40.0\n",
      "11        0          2.0     38.0\n",
      "12        0          3.0     44.0\n",
      "13        0          NaN     37.0\n",
      "14        0          3.0     40.0\n",
      "15        0          1.0     39.0\n",
      "16        0          1.0     43.0\n",
      "17        0          3.0     38.0\n",
      "18        0          2.0      NaN\n",
      "19        0          3.0     39.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and Parse Training Data\n",
    "\n",
    "train_file = pathlib.Path(\"data-science-insurance_data/train.csv\");\n",
    "\n",
    "train_df = pandas.read_csv(train_file);\n",
    "\n",
    "print(\"Training Data:\");\n",
    "print(\">  Size:\", train_df.shape);\n",
    "print(train_df.head(20));\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data:\n",
      ">  Size: (5, 2)\n",
      "   categorical  numeric\n",
      "0          NaN     71.0\n",
      "1          3.0     75.0\n",
      "2          NaN     71.0\n",
      "3          1.0      NaN\n",
      "4          2.0     73.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and Parse Test Data\n",
    "\n",
    "test_file = pathlib.Path(\"data-science-insurance_data/test.csv\");\n",
    "\n",
    "test_df = pandas.read_csv(test_file);\n",
    "\n",
    "print(\"Testing Data:\");\n",
    "print(\">  Size:\", test_df.shape);\n",
    "print(test_df.head(20));\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalised Linear Model (GLM)\n",
    "\n",
    "A __Generalised Linear Model__ (GLM) is a linear-system-of-equations, regression approach to estimating the functional relationship between input (independent) variables and an output (dependent) variable. This is also referred to as [Generalised Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression) in some libraries, like `sklearn` (`scikit-learn`).\n",
    "\n",
    "To discuss this in context, let's first establish the algebraic symbols for the data that we have (from the previous cell).\n",
    "\n",
    "Independent Variables:\n",
    "\n",
    "- $x_{0}$: `categorical`\n",
    "- $x_{1}$: `numeric`\n",
    "\n",
    "Dependent Variable:\n",
    "\n",
    "- $y$: `outcome`\n",
    "\n",
    "$y = f\\left(x_{0}, x_{1}\\right)$\n",
    "\n",
    "So, from the above equation, we have our task at hand. We need to figure out what the relationship is between the $x_{i}$ variables and the `outcome` ($y$) variable. We don't have any other information, but we were directed to use a __GLM__ approach. In general, we would plot data in 2D or 3D, if possible, and try to recognise some shape to the spread of the data. If there seemed to be something line-like or plane-like, we could then justify the use of a linear model. But, since we were told what approach to take, we'll leave the justification as being outside the scope of this work.\n",
    "\n",
    "A __Generalised Linear Model__ (GLM) is typically written through a Classical Linear Algebra relationship between all the samples, but we can be a bit more clear and write out the full equation for one sample (row $i$ of the `train_df` DataFrame):\n",
    "\n",
    "$\\hat{y_{i}} = g^{-1}\\left(\\beta_{0} + \\beta_{1}{\\cdot}x_{i,0} + \\beta_{2}{\\cdot}x_{i,1} + \\epsilon_{i}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> GLM Parameters:\n",
      "  - alpha : 1.0\n",
      "  - fit_intercept : True\n",
      "  - link : log\n",
      "  - max_iter : 100\n",
      "  - power : 1\n",
      "  - tol : 0.0001\n",
      "  - verbose : 0\n",
      "  - warm_start : False\n",
      "\n",
      "> Done!\n"
     ]
    }
   ],
   "source": [
    "# In sklearn a Tweedie Regressor with `link=\"log\"`\n",
    "#  and `power=1` is equivalent to a log-Link-Function\n",
    "#  Poission-distribution Generalised Linear Model.\n",
    "# https://scikit-learn.org/stable/modules/linear_model.html#usage\n",
    "\n",
    "from sklearn.linear_model import TweedieRegressor;\n",
    "\n",
    "regressor_obj = TweedieRegressor(\n",
    "    power= 1,              # 1: Poisson Distribution\n",
    "    link= \"log\",           # Link Function\n",
    "    alpha= 1.0,            # Default: 1.0\n",
    "    tol= 1e-4,             # Default: 1e-4\n",
    "    fit_intercept= True,   # Default: True\n",
    "    max_iter= 100,         # Default: 100\n",
    "    verbose= 0,            # Default: 0\n",
    "    warm_start= False,     # Default: False\n",
    ");\n",
    "\n",
    "# Regressor doesn't support `NaN`, so we need to\n",
    "#  either drop or replace the values. As a first\n",
    "#  pass, let's just do something \"naïve\" like\n",
    "#  replace the `NaN` values with the Arithmetic\n",
    "#  Mean value from their respective column.\n",
    "\n",
    "cleaned_train_df = train_df.fillna(\n",
    "    train_df.mean(),\n",
    "    inplace= False,\n",
    ");\n",
    "\n",
    "# Since we don't have anything to say that we\n",
    "#  should weight any of the samples differently\n",
    "#  from one another, then we can just set the\n",
    "#  `sample_weight` parameter ot the default\n",
    "#  value of `None`, so that they're all weighed\n",
    "#  equally.\n",
    "\n",
    "regressor_obj.fit(\n",
    "    cleaned_train_df[[\"categorical\", \"numeric\",]],\n",
    "    cleaned_train_df[\"outcome\"],\n",
    "    sample_weight= None,\n",
    ");\n",
    "\n",
    "print(\"> GLM Parameters:\");\n",
    "\n",
    "regressor_params_dct = regressor_obj.get_params();\n",
    "\n",
    "for key in regressor_params_dct:\n",
    "    print(\"  -\", key, \":\", regressor_params_dct[key]);\n",
    "# rof\n",
    "\n",
    "print(\"\");\n",
    "print(\"> Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique `outcome` Values:\n",
      "[0 1 3 2 5 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Range of Outcome Values\n",
    "\n",
    "print(\"Unique `outcome` Values:\");\n",
    "print(train_df[\"outcome\"].unique());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicitons for `outcome`:\n",
      ">  Size: (5, 4)\n",
      "   categorical  numeric  outcome_raw  outcome_rounded\n",
      "0          NaN     71.0     2.431978                2\n",
      "1          3.0     75.0     3.592392                4\n",
      "2          NaN     71.0     2.431978                2\n",
      "3          1.0      NaN     0.424782                0\n",
      "4          2.0     73.0     2.919786                3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outcome Prediction using the \"fit\"-ed GLM\n",
    "\n",
    "predicted_df = test_df.copy(deep= True,);\n",
    "predicted_df[\"outcome_raw\"] = -1;\n",
    "predicted_df[\"outcome_rounded\"] = -1;\n",
    "\n",
    "# Again, sklearn cannot handle `NaN` values, so\n",
    "#  we can again use the \"naïve\" approach, but\n",
    "#  instead of using the Arithmetic Means of the\n",
    "#  `test_df` columns, we still want to use the\n",
    "#  Means from the `train_df` columns, so that\n",
    "#  we don't add extra bias on top of whatever\n",
    "#  bias we're already injecting. Again, this is\n",
    "#  a non-ideal approach, but it gives us a\n",
    "#  way to get a quick output to review the\n",
    "#  predictions and see if we need to tweak the\n",
    "#  parameters of our GLM.\n",
    "\n",
    "cleaned_test_df = test_df.fillna(\n",
    "    train_df.mean(),\n",
    "    inplace= False,\n",
    ");\n",
    "\n",
    "predicted_df[\"outcome_raw\"] = regressor_obj.predict(\n",
    "    cleaned_test_df[[\"categorical\", \"numeric\",]],\n",
    ");\n",
    "\n",
    "predicted_df[\"outcome_rounded\"] = predicted_df[\"outcome_raw\"].round(\n",
    "    decimals= 0,\n",
    ").astype(int);\n",
    "\n",
    "print(\"Predicitons for `outcome`:\");\n",
    "print(\">  Size:\", predicted_df.shape);\n",
    "print(predicted_df.head(20));\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> GLM Parameters:\n",
      "  - alpha : 0.15\n",
      "  - fit_intercept : True\n",
      "  - link : log\n",
      "  - max_iter : 50000\n",
      "  - power : 1\n",
      "  - tol : 1e-06\n",
      "  - verbose : 0\n",
      "  - warm_start : False\n",
      "\n",
      "Predicitons for `outcome`:\n",
      ">  Size: (5, 4)\n",
      "   categorical  numeric  outcome_raw  outcome_rounded\n",
      "0          NaN     71.0     2.492728                2\n",
      "1          3.0     75.0     3.962121                4\n",
      "2          NaN     71.0     2.492728                2\n",
      "3          1.0      NaN     0.393089                0\n",
      "4          2.0     73.0     3.004424                3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Great! We got an output, but let's try again\n",
    "#  with some tweaks to the GLM parameters and\n",
    "#  see how accurate/stable it is...\n",
    "\n",
    "regressor_obj = TweedieRegressor(\n",
    "    power= 1,              # 1: Poisson Distribution\n",
    "    link= \"log\",           # Link Function\n",
    "    alpha= 0.15,           # Default: 1.0\n",
    "    tol= 1e-6,             # Default: 1e-4\n",
    "    fit_intercept= True,   # Default: True\n",
    "    max_iter= 50000,       # Default: 100\n",
    "    verbose= 0,            # Default: 0\n",
    "    warm_start= False,     # Default: False\n",
    ");\n",
    "\n",
    "regressor_obj.fit(\n",
    "    cleaned_train_df[[\"categorical\", \"numeric\",]],\n",
    "    cleaned_train_df[\"outcome\"],\n",
    "    sample_weight= None,\n",
    ");\n",
    "\n",
    "print(\"> GLM Parameters:\");\n",
    "\n",
    "regressor_params_dct = regressor_obj.get_params();\n",
    "\n",
    "for key in regressor_params_dct:\n",
    "    print(\"  -\", key, \":\", regressor_params_dct[key]);\n",
    "# rof\n",
    "\n",
    "print(\"\");\n",
    "\n",
    "predicted_df[\"outcome_raw\"] = regressor_obj.predict(\n",
    "    cleaned_test_df[[\"categorical\", \"numeric\",]],\n",
    ");\n",
    "\n",
    "predicted_df[\"outcome_rounded\"] = predicted_df[\"outcome_raw\"].round(\n",
    "    decimals= 0,\n",
    ").astype(int);\n",
    "\n",
    "print(\"Predicitons for `outcome`:\");\n",
    "print(\">  Size:\", predicted_df.shape);\n",
    "print(predicted_df.head(20));\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Discussion\n",
    "\n",
    "So, we did a default __GLM__ with a __Tweedie Regressor__ configured with the _log_-Link-Function and Poisson Distribution as directed. We got the results of `[2, 4, 2, 0, 3]` for the predicted `outcome`, and then we set the `alpha` (step-size) really small to `0.15` and upped the iterations to `50,000` while lowering the tolerence to `1e-6`.\n",
    "\n",
    "When we comapre the `outcome_raw` values we see some asymptotic behaviours of how the values are tendending essentially towards: `[2.5, 4.0, 2.5, 0.4, 3.0]`.\n",
    "\n",
    "This may mean that our answer of `[2, 4, 2, 0, 3]` is only slightly more likely than `[3, 4, 3, 0, 3]`, `[2, 4, 3, 0, 3]`, or `[3, 4, 2, 0, 3]`, as the raw values of `2.5` could round to either `2` or `3`.\n",
    "\n",
    "Since we don't know the true `outcome` values for the `test.csv` data, we can try computing the $D^{2}$ scores with the `train.csv` data, to get an estimate of what is sorta kinda basically the \"residual (error) variance\". Since we're using known related values, there's a semantic argument of whether these are residuals or errors, but that's not really necessary to clarify here.\n",
    "\n",
    "Note that if $d(\\dots)$ is the [\"Deviance\" Function](https://en.wikipedia.org/wiki/Deviance_(statistics)), $D^{2}$ is defined as:\n",
    "\n",
    "$$\n",
    "D^{2} = 1 - \\frac{d\\left(y, \\hat{y}\\right)}{d_{null}}\n",
    "$$\n",
    "\n",
    "Which basically gives an optimal value of $1$ if the deviance between the actual and predicated values is zero, and otherwise is less than $1$ (including negative values). The difference from $1$ is the ratio between the deviance of the predicted ($\\hat{y}$) values from the real values ($y$) and the \"null deviance\", which is the deviance when assuming that the Linear Model (the line/hyperplane) fits to the Average line (hyperplane) going through the data \"cloud\".\n",
    "\n",
    "So, let's see what the `train.csv` $D^{2}$ scores are using the second set of parameters for the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D^2 Score: 0.2905\n"
     ]
    }
   ],
   "source": [
    "d2_score = regressor_obj.score(\n",
    "    cleaned_train_df[[\"categorical\", \"numeric\",]],\n",
    "    cleaned_train_df[\"outcome\"],\n",
    "    sample_weight= None,\n",
    ");\n",
    "\n",
    "print(\n",
    "    \"D^2 Score:\",\n",
    "    \"{0:0.4f}\".format(d2_score),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $D^{2}$ Score\n",
    "\n",
    "A score of `0.29` indicates that the ratio $\\frac{d\\left(y, \\hat{y}\\right)}{d_{null}}$ is equal to `0.71`, indicating that our regression \"line\" is (on average) about 30% off of the simple (y-intercept-only Model) Average line through the data.\n",
    "\n",
    "Ideally, we'd want a value much closer to `1`, so this isn't a great score, indicating that probably a __Generalised Linear Model__ (GLM) isn't very accurate for this data.\n",
    "\n",
    "Obviously, we've also skewed the data through our \"imputations\" by filling in all the `NaN` values with training data averages.\n",
    "\n",
    "A follow-up to this could be to see how much the predictions and $D^{2}$ score change if we dropped all the samples with `NaN` values, and then only used the averages to fill in the `test.csv` data.\n",
    "\n",
    "We should also point out that our predicted values that were close to `0.4` or `0.5` were actually the rows of `test.csv` that had `NaN` values. This likely means that our Arithmetic Mean values poorly skewed the data, since the other two samples were basically right on with `4.0` and `3.0` values. Through better inspection of the data (plotting, simple statistics, Kernel Density Estimation, _etc._), we could probably come up with a better \"imputing\" (missing-data filling) approach that would hopefully more strongly bias our predictions towards more \"realistic\" outcomes.\n",
    "\n",
    "As we mentioned above, values like `2.5` could either be `2` or `3`, with no real indication towards one or the other. In this case, we would __want__ to have biasing towards `2` or `3`, to make the prediction more \"self-assured\". The trouble with injecting/forcing the bias is that we'd need to have a good rationale behind it. That's partly why we start with the \"naïve\" approach, because we want to just see what happens when we do something simple. The Arithmetic Mean is known statistically as (very likely) being \"unbiased\", such that its distribution is equivalent to the true (population) Mean's distribution. As such, we're not likely to be skewing one way or the other, away from truth, when filling in data with the Arithmetic Mean.\n",
    "\n",
    "However, the `numeric` and `categorical` means and distributions are likely very different, along with their ranges. So depending on which column we're filling, we're skewing our prediction better or worse, depending on how well our predictor performs for that indpendent variable. And that's where the deeper introspection would come into play. We'd want to figure out how good our linear fit is (our regression model) and adjust it as necessary, to move (bias) our predictor towards \"better\" predictions. And, as such, we'd want to make sure that values we fill in don't overwhelm the pre-existing data. So, if one independent variable is more variant than the other, we'd want to bias our \"imputations\" (filled-in values) to fight against that variance so that we get something more realistic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
