{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Data Science (Marketing)\" Simple Challenge\n",
    "\n",
    "We found a company's publicly-hosted GitHub Account with simple pre-interview challenges for several Data-Science-related roles. We removed the company information and created this notebook to practice the challenges.\n",
    "\n",
    "The data is theirs, but all code is our creation. The repository was provided under the MIT License, so while technically we've broken the license by not including it, we'd rather keep the anonymity and disassociation since the MIT License is free and permissive.\n",
    "\n",
    "All dependencies (`import` calls) for this notebook are in the first code-block.\n",
    "\n",
    "Alongside this notebook is a lock-file generated using the `poetry` package, which has all the exact dependencies used to generate and run this Notebook. The `pyproject.toml` file is the project configuration file generated by `poetry` -- which is installable with `pip`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Challenge\n",
    "\n",
    "<style type=\"text/css\">\n",
    "    ol ol { list-style-type: lower-alpha; }\n",
    "</style>\n",
    "\n",
    "_(The following has been extracted from a PDF at the originating company's GitHub repository of challenges. The associated data with the challenge are in the `data-science-marketing_data` directory.)_\n",
    "\n",
    "Success in the simple challenge leads to the final two steps of the interview process:\n",
    "\n",
    "1. Informal chat with [Company] founders\n",
    "1. Full technical challenge\n",
    "\n",
    "For the simple challenge, use `train.csv` to predict the likelihood of the `outcome` variable being equal to `1`.\n",
    "\n",
    "__Requirements:__\n",
    "\n",
    "1. All code must be written in Python and must be in a Jupyter notebook\n",
    "1. The first cell in the notebook must include:\n",
    "  1. Your last name (please don’t include any other identifying information)\n",
    "  1. The date\n",
    "  1. A one sentence description of your approach\n",
    "  1. The estimated AUC you would expect to get on the `test.csv` data. If your estimated AUC is less than `.825`, your submission will not be reviewed.\n",
    "1. Your code _must_ be able to predict _all_ observations in the test dataset. The last cell in the notebook must output the first five predicted values of the `outcome` variable for `test.csv`.\n",
    "\n",
    "If you are spending more than an hour on this simple challenge because there are so many things you want to demonstrate, you are spending too much time on it. If you are spending more than an hour on it because you don’t know where to start, please be warned that the full technical challenge will be considerably more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated AUC\n",
    "\n",
    "A __Receiver Operating Characteristic__ (ROC) curve is the __True Positive Rate__ (TPR) as a function of the __False Positive Rate__ (FPR) for a given \"threshold\" in a binary-classification.\n",
    "\n",
    "In a binary-classification, a __Confusion Matrix__ is a `2x2` array of the __True Positive__ (TP), __True Negative__ (TN), __False Positive__ (FP), and __False Negative__ (FN) counts.\n",
    "\n",
    "The __True Positive Rate__ (TPR) is formulated as: `TP / (TP + FN)`.\n",
    "\n",
    "The __False Positive Rate__ (FPR) is formulated as: `FP / (FP + TN)`.\n",
    "\n",
    "Per the publication \"An introduction to ROC analysis\" by Tom Fawcett (Pattern Recognition Letters, 2006), all ROC curves have a start-point at `(FPR, TPR) = (0, 0)` and `(1.0, 1.0)`. For any binary-classification \"threshold\" (decision point), we will end up with a new `(FPR(t), TPR(t))` point. With only 3 points, we can calculate a rough area for an __Area Under the Curve__ (AUC) estimate as:\n",
    "\n",
    "`AUC = ((TPR(t) * FPR(t)) / 2) + (((1 - TPR(t)) * (1 - FPR(t))) / 2) + (TPR(t) * (1 - FPR(t)))`\n",
    "\n",
    "Which can be rewritten as:\n",
    "\n",
    "`AUC = (TPR(t) * FPR(t) * (1/2 + 1/2 - 1)) + 1/2 - (TPR(t)/2) - (FPR(t)/2) + TPR(t)`\n",
    "\n",
    "And that simplifies to:\n",
    "\n",
    "`AUC = (TPR(t) - FPR(t) + 1) / 2`\n",
    "\n",
    "### Estimate\n",
    "\n",
    "> _Filled-in retroactively after building notebook. We don't have the \"actual\" classifications for the `test.csv`, so this AUC estimate is based-on how the `train.csv` performed in a self-analysis after classification, with more discussion below._\n",
    "\n",
    "For our trained classifier, our estimate of AUC is at least `0.88`, and up to `0.99`, most likely around `0.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Python v3.8.2 (default, Apr 13 2020, 19:02:26) \n",
      "[Clang 11.0.3 (clang-1103.0.32.29)]\n",
      "\n",
      ">> Loading: pandas v1.1.0\n",
      ">> Loading: plotly v4.9.0\n",
      ">> Loading: sklearn v0.23.2\n",
      "\n",
      ">> Dependencies Loaded.\n"
     ]
    }
   ],
   "source": [
    "# Python Standard Library\n",
    "import pathlib;\n",
    "import sys;\n",
    "\n",
    "# Third-Party Packages\n",
    "import pandas;\n",
    "\n",
    "import plotly;\n",
    "import plotly.express as plotly_express;\n",
    "\n",
    "import sklearn;\n",
    "\n",
    "print(\">> Python v{0:s}\".format(sys.version));\n",
    "print(\"\");\n",
    "print(\">> Loading: pandas v{0:s}\".format(pandas.__version__));\n",
    "print(\">> Loading: plotly v{0:s}\".format(plotly.__version__));\n",
    "print(\">> Loading: sklearn v{0:s}\".format(sklearn.__version__));\n",
    "print(\"\");\n",
    "print(\">> Dependencies Loaded.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   age  cost_of_ad device_type gender  in_initial_launch_location  income  \\\n",
      "0   56    0.005737      iPhone      M                           0   62717   \n",
      "1   50    0.004733     desktop      F                           0   64328   \n",
      "2   54    0.004129      laptop      M                           0   83439   \n",
      "3   16    0.005117     Android      F                           0   30110   \n",
      "4   37    0.003635     desktop      M                           0   76565   \n",
      "\n",
      "   n_drivers  n_vehicles  prior_ins_tenure  outcome  \n",
      "0          2           1                 4        0  \n",
      "1          2           3                 2        0  \n",
      "2          1           3                 7        0  \n",
      "3          2           3                 0        0  \n",
      "4          2           1                 5        0  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and Parse Training Data\n",
    "\n",
    "train_file = pathlib.Path(\"data-science-marketing_data/train.csv\");\n",
    "\n",
    "train_df = pandas.read_csv(train_file);\n",
    "\n",
    "print(\"Training Data:\");\n",
    "print(train_df.head());\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Classifier... done!\n"
     ]
    }
   ],
   "source": [
    "# Create Gradient-Boosted Classifier\n",
    "\n",
    "import xgboost;\n",
    "\n",
    "device_sr = train_df[\"device_type\"];\n",
    "device_enc = sklearn.preprocessing.LabelEncoder();\n",
    "device_lbl = device_enc.fit(device_sr.unique());\n",
    "device_lbl = device_enc.transform(device_sr);\n",
    "\n",
    "gender_sr = train_df[\"gender\"].apply(str);\n",
    "gender_enc = sklearn.preprocessing.LabelEncoder();\n",
    "gender_lbl = gender_enc.fit(gender_sr.unique());\n",
    "gender_lbl = gender_enc.transform(gender_sr);\n",
    "\n",
    "train_enc_df = train_df.copy(deep= True,);\n",
    "train_enc_df[\"device_type\"] = device_lbl;\n",
    "train_enc_df[\"gender\"] = gender_lbl;\n",
    "\n",
    "outcome_sr = train_enc_df[\"outcome\"];\n",
    "train_enc_df.drop(\n",
    "    columns= [\"outcome\",],\n",
    "    axis= 1,\n",
    "    inplace= True,\n",
    ");\n",
    "\n",
    "train_dm = xgboost.DMatrix(\n",
    "    data= train_enc_df,\n",
    "    label= outcome_sr,\n",
    ");\n",
    "\n",
    "crossval_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"max_depth\": 25,\n",
    "    \"eta\": 0.5,\n",
    "    \"objective\": \"binary:hinge\",\n",
    "};\n",
    "\n",
    "classifier_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"max_depth\": 25,\n",
    "    \"eta\": 0.5,\n",
    "    \"objective\": \"binary:hinge\",\n",
    "};\n",
    "num_round = 20;\n",
    "\n",
    "cross_val = xgboost.cv(\n",
    "    crossval_params,\n",
    "    train_dm,\n",
    "    20, # nrounds\n",
    "    20, # nfold\n",
    ");\n",
    "\n",
    "classifier_obj = xgboost.train(\n",
    "    params= classifier_params,\n",
    "    dtrain= train_dm,\n",
    "    num_boost_round= num_round,\n",
    ");\n",
    "\n",
    "print(\"Trained Classifier... done!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xgboost.core.Booster object at 0x11ba9b790>\n",
      "'False' Outcome: 0\n",
      " 'True' Outcome: 1\n",
      "\n",
      "Classified Training Data AUC Thresholds:\n",
      "[2. 1. 0.]\n",
      "\n",
      "Classified Training Data Confusion-Matrix:\n",
      "[[9018    0]\n",
      " [  17  965]]\n",
      "\n",
      "Classified Training Data AUC:\n",
      "0.9913441955193483\n",
      "\n",
      "Classified Training Data Cross-Validation:\n",
      "    train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0           0.901800         0.000863           0.9018        0.016394\n",
      "1           0.014947         0.001180           0.1230        0.011841\n",
      "2           0.014942         0.001179           0.1230        0.011841\n",
      "3           0.014942         0.001179           0.1230        0.011841\n",
      "4           0.014942         0.001179           0.1230        0.011841\n",
      "5           0.014832         0.001174           0.1227        0.011336\n",
      "6           0.014516         0.001088           0.1223        0.012054\n",
      "7           0.011884         0.001266           0.1211        0.011322\n",
      "8           0.010863         0.001383           0.1214        0.012002\n",
      "9           0.009416         0.001491           0.1217        0.011718\n",
      "10          0.008474         0.001684           0.1232        0.012123\n",
      "11          0.007368         0.001958           0.1236        0.012110\n",
      "12          0.006726         0.001945           0.1223        0.009965\n",
      "13          0.005900         0.002230           0.1228        0.012983\n",
      "14          0.005326         0.002254           0.1241        0.012255\n",
      "15          0.004742         0.002260           0.1243        0.012558\n",
      "16          0.004253         0.002218           0.1254        0.011901\n",
      "17          0.003989         0.002289           0.1227        0.012285\n",
      "18          0.003553         0.002185           0.1237        0.012219\n",
      "19          0.003458         0.002119           0.1215        0.010371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_file = pathlib.Path(\"data-science_data/test.csv\");\n",
    "\n",
    "test_df = pandas.read_csv(test_file);\n",
    "\n",
    "test_device_lbl = device_enc.transform(test_df[\"device_type\"]);\n",
    "test_gender_lbl = gender_enc.transform(test_df[\"gender\"].apply(str));\n",
    "\n",
    "test_enc_df = test_df.copy(deep= True,);\n",
    "test_enc_df[\"device_type\"] = test_device_lbl;\n",
    "test_enc_df[\"gender\"] = test_gender_lbl;\n",
    "\n",
    "test_dm = xgboost.DMatrix(test_enc_df);\n",
    "\n",
    "print(classifier_obj)\n",
    "\n",
    "train_classified = classifier_obj.predict(train_dm);\n",
    "test_classified = classifier_obj.predict(test_dm);\n",
    "\n",
    "classes = outcome_sr.unique();\n",
    "\n",
    "print(\"'False' Outcome:\", classes[0]);\n",
    "print(\" 'True' Outcome:\", classes[1]);\n",
    "print(\"\");\n",
    "\n",
    "train_conmat = sklearn.metrics.confusion_matrix(outcome_sr, train_classified);\n",
    "train_fpr, train_tpr, train_thresholds = sklearn.metrics.roc_curve(\n",
    "    outcome_sr,\n",
    "    train_classified,\n",
    "    pos_label= classes[1],\n",
    ");\n",
    "train_auc = sklearn.metrics.auc(train_fpr, train_tpr);\n",
    "\n",
    "print(\"Classified Training Data AUC Thresholds:\");\n",
    "print(train_thresholds);\n",
    "print(\"\");\n",
    "print(\"Classified Training Data Confusion-Matrix:\");\n",
    "print(train_conmat);\n",
    "print(\"\");\n",
    "print(\"Classified Training Data AUC:\");\n",
    "print(train_auc);\n",
    "print(\"\");\n",
    "print(\"Classified Training Data Cross-Validation:\");\n",
    "print(cross_val);\n",
    "print(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "For classification we used a `binary:hinge` classifier through a Gradient-Boosted-Trees algorithm, provided by `xgboost`'s Python API.\n",
    "\n",
    "After setting-up the classifier and doing a naïve evaluation, we found that our training AUC was only `0.50`, which meant out classifier was meaningless.\n",
    "\n",
    "We made a few changes to the `classifier_params` dictionary and the `num_rounds` value, but settled on the current values.\n",
    "\n",
    "This was an `ad hoc` approach, though not without reasonably well-educated guesses.\n",
    "\n",
    "We increased `num_rounds` and the `max_depth` to give the algorithm more iterations to train itself. For the sake of brevity we removed the plots, but we did a preliminary evaluation of the histograms of each column in the training dataset, and most of the \"distributions\" were relatively flat. This provided a visual means of estimating Entropy, and relatively flat distributions have very low Entropy, meaning that those columns would provide minimal (Shannon) Information and would be unlikely to be useful in training a classifier. Of course, the combination of \"features\" could prove to have sufficient Entropy to make a discriminations in a classifier, which is why we chose the `xgboost` library.\n",
    "\n",
    "Using the `binary:hinge` \"objective\" was a choice of convenience, to simply get `0` or `1` labels that easily correspond to the `outcome` labels.\n",
    "\n",
    "We also found that `eta` seemed to have a detrimental effect at values below and above `0.5`, though lower was worse.\n",
    "\n",
    "As you can see, above, we achieved an AUC of `0.99`, which is shown in the Confusion Matrix as well.\n",
    "\n",
    "We produced `test_classified` (the classifier run on the `test` data), but we obviously could not evaluate its AUC or any Confusion Matrix because it did not come with any ground-truth `outcomes`.\n",
    "\n",
    "Other classification approaches could've worked, or even the same approach with a `binary:logistic` \"objective\" could've been likely more informative if we were to go more in-depth about the potential accuracy of our predictions.\n",
    "\n",
    "We also used the XGBoost __Cross Validation__ algorithm to do a `K = 20`, K-Fold __Cross Validation__ of the classifier (same configuration, at least, if not technically the same exact classifier). This showed after 20 rounds that the expected Test Data __Accuracy__ would be around `0.88`, which should be roughly the same as the AUC. Since the K-Fold __Cross Validation__ is randomly paritioning the original data and training on the smaller subsets, we see also that the trained accuracy improves to `0.997` when we got an AUC of at least `0.99`. So there was potentially an outlier-rejection benefit to the partitioning, which may be misleading. Again, the __AUC__ and the complement of the average error in __Cross Validation__ aren't the same values, but they're certainly correlated.\n",
    "\n",
    "As such, and as stated above, we expect that our predictor AUC for the Test Data would bottom-out around `0.88`, provided that the training data is very well indicative of the test data. But realistically, under that same assumption, we'd expect an AUC of closer to `0.95` given the very high accuracy of our trained classifier, here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
